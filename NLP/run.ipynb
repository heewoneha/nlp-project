{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ab2a49-421e-46d2-bbee-83e257b82507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction, EarlyStoppingCallback \\\n",
    "    , AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from plugin import delete_folder, check_exists_cuda, load_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a393807-9a10-4a23-811f-b01d2053500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "original_aspects = [ # 나중에 데이터로부터 불러와서 자동으로 만들어보자 !\n",
    "    '가격', '기능/효과', '디자인',\n",
    "    '발림성', '보습력/수분감/쿨링감', '사용감',\n",
    "    '성분', '용기', '용량', '유통기한',\n",
    "    '윤기/피부(톤)', '자극성', '제품구성', '제형',\n",
    "    '지속력/유지력', '편의성/활용성',\n",
    "    '품질', '피부타입', '향', '흡수력'\n",
    "]\n",
    "\n",
    "category_with_original_aspects = [ # main category 받아서 하도록 나중에 바꾸기 !\n",
    "    f'남성화장품#{aspect}' for aspect in original_aspects\n",
    "]\n",
    "\n",
    "sentiment_id_to_str = ['1', '-1', '0']  # pos: 0, neg: 1, neu: 2로 변환\n",
    "sentiment_str_to_id = {sentiment_id_to_str[i]: i for i in range(len(sentiment_id_to_str))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be64f827-4d17-4157-afef-825c6c1e3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class klue_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Input: 정규표현식, 개수가 적은 속성 제거 등으로 전처리된 데이터셋\n",
    "    Ouput: 1차원 텐서(__getitem__) / 샘플의 수(__len__)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, label):\n",
    "        self.dataset = dataset # {'input_ids': ~, 'token_type_ids': ~, 'attention_mask': ~, 'entity_ids' : ~}\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439fc4eb-45bc-4af9-ac98-2262cf2e75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    seed value를 고정하는 함수\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56dce01-99d7-4622-b869-f7f73b03b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_datasets(data, main_category): # train, validation, test별로 들어옴 !\n",
    "    \"\"\"\n",
    "    속성 & 감성 관련 데이터 및 레이블을 정의하는 함수\n",
    "    \"\"\"\n",
    "    ASP_datas = [[] for i in range(len(original_aspects))]\n",
    "    ASP_labels = [[] for i in range(len(original_aspects))]\n",
    "\n",
    "    SEN_data = []\n",
    "    SEN_labels = []\n",
    "\n",
    "    for i, pair in enumerate(category_with_original_aspects):\n",
    "        for datas in data:\n",
    "            review = datas['raw_text']\n",
    "            annotations = datas['annotation']\n",
    "            check_point = False\n",
    "            \n",
    "            ASP_datas[i].append(review)\n",
    "            \n",
    "            for annotation in annotations:\n",
    "                entity_property = f'{main_category}#' + annotation[0]\n",
    "                sentiment = annotation[1]\n",
    "\n",
    "                if entity_property == pair:\n",
    "                    check_point = True\n",
    "                    \n",
    "            if check_point:\n",
    "                ASP_labels[i].append(1)\n",
    "                SEN_data.append(review + \" \" + pair)\n",
    "                SEN_labels.append(sentiment_str_to_id[sentiment])\n",
    "            \n",
    "            else:\n",
    "                ASP_labels[i].append(0)\n",
    "                \n",
    "        ASP_datas[i], ASP_labels[i] = shuffle(ASP_datas[i], ASP_labels[i], random_state = 42)\n",
    "        \n",
    "    SEN_data, SEN_labels = shuffle(SEN_data, SEN_labels, random_state = 42)\n",
    "    \n",
    "    return ASP_datas, ASP_labels, SEN_data, SEN_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e363f7e-e8c8-4f90-a75e-ce39f065f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_1d(val, Datas, labels, tokenizer): # train, validation, test별로 들어옴 !\n",
    "    \"\"\"\n",
    "    Class를 이용해 1차원 텐서로 변경하는 함수\n",
    "    \"\"\"\n",
    "    if val == 'aspect':\n",
    "        klue_sets = []\n",
    "\n",
    "        for i in range(len(category_with_original_aspects)):\n",
    "            tok_sentence = tokenizer(Datas[i], return_tensors=\"pt\", padding='max_length' \\\n",
    "                            , truncation=True, max_length=256, add_special_tokens=True)  \n",
    "            \n",
    "            klue_sets.append(klue_Dataset(tok_sentence, labels[i]))\n",
    "        \n",
    "        return klue_sets\n",
    "    \n",
    "    elif val == 'sentiment':\n",
    "        sen_tok_sentence = tokenizer(Datas, return_tensors=\"pt\", padding='max_length' \\\n",
    "                            , truncation=True,max_length=256, add_special_tokens=True)  \n",
    "        \n",
    "        SEN_klue_sets = klue_Dataset(sen_tok_sentence, labels)\n",
    "\n",
    "        return SEN_klue_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b060058-6fdc-4867-96bb-2e0d2f77988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(val, p: EvalPrediction):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      val이 aspect라면    average = 'binary',  (이진 분류)\n",
    "      val이 sentiment라면 average = 'weighted' (다중클래스 분류)\n",
    "    Output:\n",
    "      평가지표 점수\n",
    "    \"\"\"\n",
    "    if val == 'aspect':\n",
    "        average = 'binary'\n",
    "    elif val == 'sentiment':\n",
    "        average = 'weighted'\n",
    "    \n",
    "    labels = p.label_ids\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=average)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843ef46b-f986-4f02-bfbd-21b011a5fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_evaluation(val, infer_labels, infers):\n",
    "    \"\"\"\n",
    "    속성/감성 모델의 테스트 평가지표 결과를 출력하는 함수\n",
    "    \"\"\"\n",
    "    if val == 'aspect':\n",
    "        length = len(category_with_original_aspects)\n",
    "        average = 'binary'\n",
    "    elif val == 'sentiment':\n",
    "        length = 1\n",
    "        average = 'weighted'\n",
    "    \n",
    "    for x in range(0, length):\n",
    "        print(x, \"th Test.....\")\n",
    "        labelss = []\n",
    "        for i in infer_labels[x]:\n",
    "            for j in i:\n",
    "                labelss.append(j)\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labelss, infers[x], average=average)\n",
    "        acc = accuracy_score(labelss, infers[x])\n",
    "\n",
    "        print(\"Accuracy: \", acc)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5075e2-1e05-4118-8d80-eb7ef71d1e79",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe35cd2-d618-4251-975f-3fa5b06295e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========cuda========\n",
      "Device: <class 'torch.cuda.device'>\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "check_exists_cuda(device)\n",
    "\n",
    "set_seed(42)\n",
    "main_categories = ['남성화장품', ]\n",
    "\n",
    "# test\n",
    "main_category = main_categories[0]\n",
    "data = load_jsonl(f'./preprocessed_data/{main_category}.jsonl')\n",
    "\n",
    "data_len = len(data)\n",
    "\n",
    "trains = data[:int(data_len*0.6)]\n",
    "validations = data[int(data_len*0.6):int(data_len*0.8)]\n",
    "tests = data[int(data_len*0.8):]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': category_with_original_aspects\n",
    "}\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "train_ASP_datas, train_ASP_labels, train_SEN_data, train_SEN_labels = define_datasets(trains, main_category)\n",
    "validation_ASP_datas, validation_ASP_labels, validation_SEN_data, validation_SEN_labels = define_datasets(validations, main_category)\n",
    "test_ASP_datas, test_ASP_labels, test_SEN_data, test_SEN_labels = define_datasets(tests, main_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612d0c66-ba4e-4794-97ea-2e4ead4630c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차원 변환, aspect는 리스트 반환하고 sentiment는 단일 반환\n",
    "train_aspect_klue_sets = reshape_to_1d('aspect', train_ASP_datas, train_ASP_labels, tokenizer)\n",
    "validation_aspect_klue_sets = reshape_to_1d('aspect', validation_ASP_datas, validation_ASP_labels, tokenizer)\n",
    "test_aspect_klue_sets = reshape_to_1d('aspect', test_ASP_datas, test_ASP_labels, tokenizer)\n",
    "\n",
    "train_sentiment_klue_sets = reshape_to_1d('sentiment', train_SEN_data, train_SEN_labels, tokenizer)\n",
    "validation_sentiment_klue_sets = reshape_to_1d('sentiment', validation_SEN_data, validation_SEN_labels, tokenizer)\n",
    "test_sentiment_klue_sets = reshape_to_1d('sentiment', test_SEN_data, test_SEN_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26745072-4edf-4504-8b35-18617eae60d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not exists:  ./model_aspect_-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "1 ) Aspect Training ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='185' max='7060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 185/7060 00:31 < 19:31, 5.87 it/s, Epoch 0.26/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=============\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) Aspect Training ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m tmp_model\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_best\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# GPU Clean\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/accelerate/accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### aspect trainer\n",
    "for i in range(len(category_with_original_aspects)):\n",
    "    tmp_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    output_dir = './model_aspect_' + str(i)\n",
    "\n",
    "    pre_output_dir = './model_aspect_' + str(i-1)\n",
    "    delete_folder(pre_output_dir)\n",
    "\n",
    "    training_ars = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        save_total_limit=5,\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy='epoch',\n",
    "        metric_for_best_model = 'f1',\n",
    "        load_best_model_at_end = True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=tmp_model,\n",
    "        args=training_ars,\n",
    "        train_dataset=train_aspect_klue_sets[i],\n",
    "        eval_dataset=validation_aspect_klue_sets[i],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics = lambda x: compute_metrics(val='aspect', p=x),\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    print(\"=============\")\n",
    "    print(i+1, \") Aspect Training ...\")\n",
    "    trainer.train()\n",
    "    tmp_model.save_pretrained(output_dir + \"_best\")\n",
    "\n",
    "    # GPU Clean\n",
    "    with torch.no_grad(): tmp_model\n",
    "    del tmp_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943e87c-be1b-44f9-b5e0-c17a6ebd382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_dir = 'model_aspect_' + str(len(category_with_original_aspects)-1)\n",
    "delete_folder(final_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc65aa6-ca49-44c9-8f16-986343cad317",
   "metadata": {},
   "outputs": [],
   "source": [
    "### aspect evaluate tests\n",
    "\n",
    "infers = [[] for i in range(len(category_with_original_aspects))]\n",
    "infer_labels = [[] for i in range(len(category_with_original_aspects))]\n",
    "\n",
    "for i in range(len(category_with_original_aspects)):\n",
    "    print(\"=============\")\n",
    "    print(i+1, \") Aspect Test ...\")\n",
    "    BEST_MODEL_NAME = './model_aspect_' + str(i) + \"_best\"\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BEST_MODEL_NAME)\n",
    "    model.to(device)\n",
    "    dataloader = DataLoader(test_aspect_klue_sets[i], batch_size=4, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    output_pred = []\n",
    "    output_prob = []\n",
    "    labels = []\n",
    "\n",
    "    for z, data in enumerate(tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=data['input_ids'].to(device),\n",
    "                attention_mask=data['attention_mask'].to(device),\n",
    "                token_type_ids=data['token_type_ids'].to(device)\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        result = np.argmax(logits, axis=-1)\n",
    "        labels.append(data['label'].tolist())\n",
    "\n",
    "        output_pred.append(result)\n",
    "        output_prob.append(prob)\n",
    "\n",
    "    pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "    \n",
    "    infers[i].extend(pred_answer)\n",
    "    infer_labels[i].extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd58616-b2bc-462d-851e-ffad2da47ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print aspect test scores\n",
    "show_test_evaluation('aspect', infer_labels, infers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d925b4-16a2-48b0-8f7f-4293ed3371ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentiment trainer\n",
    "\n",
    "output_dir = './model_sentiment'\n",
    "\n",
    "tmp_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "tmp_model.resize_token_embeddings(tokenizer.vocab_size + num_added_toks)\n",
    "\n",
    "training_ars = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_total_limit=5,\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=tmp_model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_sentiment_klue_sets,\n",
    "    eval_dataset=validation_sentiment_klue_sets,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = lambda x: compute_metrics(val='sentiment', p=x),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Sentiment Training ...\")\n",
    "trainer.train()\n",
    "tmp_model.save_pretrained(output_dir + \"_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996395d9-bc99-42ae-9c20-ebb28d9ce556",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder('./model_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fa737-d3a5-4171-82ea-11b83e02423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentiment evaluate tests\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Sentiment Test ...\")\n",
    "BEST_MODEL_NAME = './model_sentiment_best'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BEST_MODEL_NAME)\n",
    "model.to(device)\n",
    "dataloader = DataLoader(test_sentiment_klue_sets, batch_size=4, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "labels = []\n",
    "\n",
    "for z, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'].to(device),\n",
    "            attention_mask=data['attention_mask'].to(device),\n",
    "            token_type_ids=data['token_type_ids'].to(device)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "    labels.append(data['label'].tolist())\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "\n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "\n",
    "infers = [pred_answer]\n",
    "infer_labels = [labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc465f-0056-4ee5-a50d-6d2685f7de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sentiment test scores\n",
    "show_test_evaluation('aspect', infer_labels, infers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
